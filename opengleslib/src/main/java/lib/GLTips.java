package lib;

import filter.img.WaterMarkFilter;
import util.GLES20Util;

public interface GLTips
{
    /**
     * 这个类仅记录一些个人的理解、认知，不是一定正确
     *
     * 1、Open GL 的坐标轴是 三维 的，x\y\z ，而且是右手法则，右手掌心面向自己，分别的正方向是 x: 大拇指向右的方向 ， y: 食指向上的方向， z: 中指指向自己的方向
     *
     *
     *
     * 2、Open GL 最终呈现在手机屏幕上的内容，其实就是近平面所能观察到的内容 (在 三维坐标系 (世界坐标系) 中 被定义的一个平面)
     *
     *      定义的 顶点坐标(x, y, z) 每个轴的取值范围，其实并非一定是 [-1, 1], 可以是 任何数值, 实际上顶点坐标对应的是 三维坐标系 中的点
     *      网上很多资料都定义 [-1, 1] 的原因 : 一般的博客代码, 近平面 的 (left, right, bottom, top) 都会定义成 (-屏幕宽高比, 屏幕宽高比, -1, 1) or (-1, 1, -1 / 屏幕宽高比, 1 / 屏幕宽高比)
     *      那么实际上, 在三维坐标系中的近平面大小就是 (-屏幕宽高比, 屏幕宽高比, -1, 1) --> (-1 * 比例, 1 * 比例, -1, 1), 这样 [-1, 1] 相对 归一化坐标而言, 总会是某一个轴上的顶点
     *
     *
     *
     * 3、但是 Open GL 是三维的，手机屏幕是二维，那么，呈现的内容必然会经过坐标系、点的映射：
     *      Open GL 三维坐标上的点，首先会被映射到 一个标准的立方体(边长：2, 三个轴的坐标范围都是 [-1, 1]){ https://blog.csdn.net/wangdingqiaoit/article/details/51589825 }
     *      其中，立方体是不变的 (这个立方体的坐标系，也叫归一化坐标), 立方体有三个轴，x'\y'\z'， 其中原点 (0, 0, 0) 是在 立方体 内部中心
     *      并且 立方体 的坐标系，是左手法则，左手手背面向自己，分别的正方向是: x: 大拇指向右的方向， y : 食指向上的方向， z: 中指指向 眼睛看东西的方向
     *
     *      ~~~~ 而 近平面 与 立方体 的关系， 我的理解是：将 近平面 压缩 进 立方体中（宽高非等比例压缩）
     *
     *      近平面的 left\right 映射到 立方体x'轴 的 [-1, 1], 近平面的 bottom\top 映射到 立方体y'轴的 [-1, 1], 近平面的 near\ 远平面 far 映射到 立方体z'轴的 [-1, 1]
     *      经过 三维 --> 二维 的变换、映射后, 在立方体中的点，就是最终要画到屏幕上的点
     *
     *
     *
     * 4、
     *  视口是一个矩形窗口区域。是OpenGL渲染操作最终显示的地方
     *  x, y 是渲染的起点，0，0 是屏幕左下角
     *  GLES20.glViewport(0, 0, width, height);
     *  float sWidthHeight = width / (float) height;
     *  Matrix.frustumM(f, 0, -10f, 10f, -10f / sWidthHeight, 10f / sWidthHeight, 3.0f, 5.0f);
     *      参数：
     *      {
     *          Matrix f: 用来接收数据的矩阵
     *          int offset: 矩阵中接收数据的起始位置偏移量
     *          // 近平面 其实是一个 矩形, ( 近平面 应该理解为 三维空间中的平面 )
     *          // 下面是决定近平面大小(面积、形状、宽高比例) 的四个参数, left\right\bottom\top 是基于 x|y 轴 的参数
     *          // 以下 4 个点都是基于 近平面 中心点讨论 ( 数值上，可以是任何数值，不一定是 -1f -> 1f 如上述例子)
     *          // x\y 轴的参数
     *          float left: 以中心点为基础的左边距 --> 负数
     *          float right: 以中心点为基础的右边距 --> 正数
     *          float bottom: 以中心点为基础的下边距 --> 负数
     *          float top: 以中心点为基础的上边距 --> 正数
     *
     *          // near\far 是基于 z 轴的参数, 只有处于 远近平面 之间的物体，才是可见的
     *          float near: 近平面 在 z 轴的位置
     *          float far: 远平面 在 z 轴的位置
     *      }
     *
     *
     *
     * 5、https://blog.csdn.net/junzia/article/details/52830604 解释 GLSL 语言
     * 其中，个人觉得比较重要的点:
     *      5-1 需要注意的是，GLSL中的向量表示竖向量，所以与矩阵相乘进行变换时，矩阵在前，向量在后（与DirectX正好相反） 建议结合 {@link OpenGLES# 维基百科 -- 矩阵} 来理解
     *      5-2
     *          纹理采样函数
     *          纹理采样函数有
     *              texture2D、texture2DProj、texture2DLod、texture2DProjLod、textureCube、textureCubeLod、
     *              texture3D、texture3DProj、texture3DLod、texture3DProjLod等。
     *
     *          texture表示纹理采样，2D表示对2D纹理采样，3D表示对3D纹理采样
     *          Lod后缀，只适用于顶点着色器采样
     *          Proj表示纹理坐标st会除以q
     *          纹理采样函数中，3D在OpenGLES2.0并不是绝对支持
     *          texture2D拥有三个参数，第一个参数表示纹理采样器。第二个参数表示纹理坐标，可以是二维、三维、或者四维。第三个参数加入后只能在片元着色器中调用，且只对采样器为mipmap类型纹理时有效。
     *
     *
     *
     * 6、对画 texture2D 图片，透视矩阵的理解，
     *
     *      前提：假设 目前有一张图片(宽: 1080  高: 540) 需要显示在手机屏幕 (宽: 1080  高: 2160) 上,
     *           顶点坐标(-1, 1, 0,
     *                   1, 1, 0,
     *                   1, -1, 0,
     *                   -1, -1, 0)
     *           sWH 是 图片的宽高比, sWidthHeight 是预览区域的宽高比 {@link GLES20Util#getFrustumM(float[], int, int, int, int)}
     *
     *      要求：图片不能被拉伸、压缩、变形
     *
     *      处理方法: 在网上看的资料，透视矩阵一般这样设置 Matrix.frustumM(projectMatrix, 0, -1, 1, -sWH / sWidthHeight, sWH / sWidthHeight, 3, 5);
     *
     *      问题(有疑惑的点): 为什么 近平面 宽是 2，高是 2 * sWH / sWidthHeight, 而不是 2 / sWidthHeight, 为什么要再 乘以一个 图片的宽高比 ？？？？？？
     *
     *      个人理解:
     *      这里需要结合 三维世界坐标 和 归一化坐标 一起来推导：
     *
     *      由于上述方法是设置 三维世界坐标系 近平面 的宽高，那么此时 不能被确定的就是 三维世界坐标系 中 近平面 的真实高度 n, 那应该如何计算出需要的高度？
     *
     *      首先我们要考虑到，当前的已知条件是：
     *           a：图片的宽高比 sWH
     *           b: 近平面(归一化坐标系中)的宽度 x, 近平面(归一化坐标系中)的高度 y
     *           c: 归一化坐标系中 x:y = sWidthHeight (一般情况下)
     *
     *      变量：
     *          1、归一化坐标系中，图片在y轴正方向的高度 Iyh;
     *
     *      分析：(以下只是讨论正方向)
     *      通过 已知条件 a, b ，求得 Iyh = x * 1 / sWH
     *      因为 三维坐标系的点都是通过 映射，跟 归一化坐标系一一对应，所以 Iyh / y = 1 / n   (其中 1 / n 的 1 是顶点坐标 y = 1)
     *      代入具体数值后求得 n = sWH / sWidthHeight;
     *
     *
     *
     * 7、 继续 第6点 的补充， 对 画 texture2D 图片，矩阵的使用理解
     *
     *     依然是 第6点 的前提、要求
     *
     *     处理方法：写死透视矩阵的大小(其实是写死 近平面 在三维坐标系的大小)，
     *     然后额外用一个矩阵，对图片进行下面的操作：
     *          三维空间的缩放(缩放成图片原来比例)
     *          三维空间的平移(由于矩阵内的值，并非通过矩阵互乘得到，是直接赋值到数组，所以平移的时候，要考虑之前是否有经过缩放操作)
     *          三维空间的旋转
     *          个人感觉这种方法比较好理解 {@link gles.Gles7View#onSurfaceChanged} or {@link WaterMarkFilter#onDrawSelf() 画底图和水印的算法}
     *
     *
     *
     * 8、 对 Frame Buffer Object (FBO) 的一些个人理解与记录：
     *
     *     在构建GLSurface View 的时候，其实已经帮我们默认配置好了一个 默认的 FBO, 一般绑定这个默认的 FBO 代码
     *      GLES20.glBindFrameBuffer(GLES20.GL_FRAMEBUFFER, 0);
     *
     *     如果代码中，需要自己控制 FBO 去绘制图像，那么最终将图像绘制到屏幕的步骤是：先将图像内容绘制到新的 FBO 挂载的 texture1 上，然后将 FBO 切换成默认的，
     *     再绑定 texture1 的id，将这张纹理再绘制一次，那么就能在屏幕上看到内容了
     *
     *
     *
     * 9、 对GLES20.glBlend... 混合模式网上资料记录：
     *
     *     在 OpenGL 里, 混合技术的工作原理是把片段着色器的结果和已经在帧缓冲区中的颜色进行混合。
     *
     *     注意： 所谓源颜色和目标颜色，是跟绘制的顺序有关的。假如先绘制了一个红色的物体，再在其上绘制绿色的物体。则绿色是源颜色，红色是目标颜色。
     *
     *     源片段的值来自于片段着色器, 目标片段的值就是已经在帧缓冲区中的值, 最终颜色=(目标颜色*目标因子)@(源颜色*源因子)，其中@表示一种运算符
     *
     *     GLES20.glBlendEquation() 设置方程式符号
     *     GLES20.glBlendFuncSeparate() 设置源片段、目标片段的因子，也就是 R\G\B\A，每个通道需要乘以的 系数
     *        ---> 例子
     *
     *              GLES20.glBlendEquation(GLES20.GL_FUNC_ADD);
     *
     *              GLES20.glBlendFuncSeparate([//src rgb通道因子]GLES20.GL_ONE, [//dst rgb通道因子]GLES20.GL_ONE_MINUS_SRC_ALPHA, [//src a通道因子]GLES20.GL_ONE, [//dst a通道因子]GLES20.GL_ONE);
     *
     *              假设源图片的 a 通道，只分两种，1 or 0
     *        对于上述设置因子的方法的理解：一般来说，每个像素都有 RBGA 四个通道，
     *          那么源图片上的 某个 H 像素中， RGB通道都以原色绘制，就是 *1，源图片 H 像素中 A 通道的值 是0，此时目标图片的 RGB 通道都 *(1-0)， 若透明度为1，则目标图片 *0
     *          这就能解决将带有部分透明部分的源图片和目标图片的混合
     *          {@link OpenGLES#天天P图 android 讲解混合模式的贴} {@link OpenGLES#湖广午王 的博客}
     *
     *
     *
     * 10、对 纹理坐标 的一些个人理解！
     *
     *     simple2D 的图片，网上资料一般用 UV 坐标表示，那么 UV 坐标是什么意思？ 简单的说，就是贴图影射到模型表面的依据。
     *     完整的说，其实应该是UVW。
     *     U和V分别是图片在显示器水平、垂直方向上的坐标，取值一般都是0~1，也 就是 ( 水平方向的第U个像素/图片宽度，垂直方向的第V个像素/图片高度 )。
     *     那W呢？W的方向垂直于显示器表面，一般 用于程序贴图或者某些3D贴图技术( 记住，确实有三维贴图这种概念！)
     *
     *     一般的定义，UV 坐标 U 的正方向朝右， V 的正方向朝上，也就是与数学上的 二维坐标系 x 轴和 y 轴方向一致，( 暂时统称为：标准 UV 坐标 )
     *     而图片起点 一般定义为 UV 坐标系原点, 所以，网上的资料都说 simple2D 图片的纹理坐标原点(0, 0) 是在 图片左下角！！！！
     *
     *     但是，在 Android 平台上，UV 坐标系，y 轴坐标的正方向，刚好是相反的，所以在 Android 平台上，纹理坐标原点(0, 0)是在图片左上角
     *
     *     问题：在 OpenGL ES 的使用过程，先将 bitmap 纹理绘制在自定义的 FBO 上，然后再将挂载在自定义 FBO 上的纹理，绘制到屏幕上时，
     *          ---> 如果不经过 y 轴翻转，会出现纹理倒过来的现象？
     *
     *     原因：在 bitmap 纹理绘制到 自定义FBO 上的时候，其实成像是正确的，并没有出现y轴翻转，因为这个时候的纹理UV坐标指向、或者映射到的是 Android 平台上的 UV 坐标系，
     *          但后续将 挂载在 FBO 上的纹理，绘制到屏幕的时候，这个 纹理 的 UV 坐标就不再是 Android 平台上的 UV 坐标，而是 标准 UV 坐标，所以纹理起点改变了，
     *          就出现了翻转的现象！！！！
     *     {@link OpenGLES#UV 坐标的理解}
     *
     *
     *
     * 11、GLES20.glClearColor(r,g,b,a); 是对 surface --> 画布做处理，如果绘制 texture 到同一个画布上，不管绘制顺序如何，texture 的纹理都不会被清除
     *
     *
     *
     * 12、对第 10 点的一点补充理解：
     *      标准 UVW 坐标 是 左下角(0, 0), 右上角(1, 1), 但android 平台, V 正方向刚好是与标准相反, 所以, 图片被读取到 GPU 当作纹理时, 对于图片而言, 起点是 左上角开始,
     *      但对于 GPU 纹理而言, 起点 依然是 左下角, 所以个人猜测, 图片在上传 GPU 的过程中, 已经做了一次反转
     *
     *
     *
     * 13、对 Matrix 的一些理解：
     *
     *
     */
}
